---
title: "PITcleanr Vignette"
author: "Kevin E. See"
date: '`r Sys.Date()`'
output:
  pdf_document:
    fig_caption: yes
    toc: yes
    toc_depth: 3
  rmarkdown::html_vignette:
    fig_caption: yes
    toc: yes
    toc_depth: 3
    toc_float: yes
vignette: |
  %\VignetteIndexEntry{PITcleanr} %\VignetteEngine{knitr::rmarkdown} %\VignetteEncoding{UTF-8}
---

```{r setUp_options, echo = F, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

# Introduction

The `PITcleanr` package was developed to help query the necessary data to fit a DABOM model (**D**am **A**dult **B**ranch **O**ccupancy **M**odel) in order to estimate adult escapement to various tributaries above a tagging location. A key assumption of a DABOM model is that fish travel along a single upstream route. Therefore, the model will fail if presented with detections by a single fish in multiple tributaries above a branching node. `PITcleanr` is designed to help clean the PIT tag detection data to identify non-linear upstream pathways and help the user determine which observations to keep.

# System requirements

`PITcleanr` relies on the following R packages which can be downloaded via [CRAN](https://cran.r-project.org/web/packages/available_packages_by_name.html) or by using the function `install.packages()`:

* `dplyr`, `lubridate`, `httr`, `purrr` and `stringr`: can all be installed by installing the `tidyverse` package

In addition, many of the various queries in `PITcleanr` require connection to the internet.

```{r}
library(tidyverse)
library(httr)
library(stringr)
library(lubridate)
library(PITcleanr)
```

# Data sources

`PITcleanr` starts with data from an adult trap database, to determine which tags are considered part of the valid tag sample, and what the trap date is for each tag. For the Lower Granite dam, the data can be retreived from the "QCI Downloads" folder on the Idaho Fish and Game's [IFWIS website](https://collaboration.idfg.idaho.gov/default.aspx). The trapping data is stored in an Microsoft Access database that is updated weekly and named "LGTrappingExportJodyW.accdb". Permission must be obtained from Idaho Fish and Game before viewing, accessing and retreiving data from the site. This database contains two tables, which can be exported as .csv files. Once this is done, `PITcleanr` has a function, `filterLGRtrapDB` to determine which tags are valid, and when they were caught in the adult trap. 

Other than the adult trap database, `PITcleanr` relies on queries from the Columbia Basin Research Data Access in Real Time ([DART](http://www.cbr.washington.edu/dart)) and the Columbia Basin PIT Tag Information System ([PTAGIS](https://ptagis.org/)). 

# Workflow

For the purposes of this vignette, we will focus on data to be fed into the Lower Granite version of DABOM, using data from spring/summer Chinook that crossed Lower Granite in the spring and summer of 2015.

```{r}
spp = 'Chinook'
yr = 2014
```


## Configuration Table

One of the first steps is to compile a configuration file containing information about all of the detection sites that may be present. `PITcleanr` will create such a configuration file for all sites in the Columbia River basin, using the `buildConfig()` function.

```{r}
org_config = buildConfig()
```

A user can then make custom changes depending on the model set-up. For example, in the Lower Granite version of DABOM, all antennas in VC1 and VC2 are combined into a single node.

```{r}
my_config = org_config %>%
  mutate(Node = ifelse(SiteID %in% c('VC2', 'VC1', 'LTR', 'MTR', 'UTR'),
                       SiteID,
                       Node),
         Node = ifelse(SiteID %in% c('CROTRP',
                                     'CRT',
                                     'REDTRP',
                                     'REDR',
                                     'RRT'),
                       'above_SC2',
                       Node),
         Node = ifelse(SiteID == 'AFC',
                       ifelse(grepl('MAINSTEM', AntennaGroup),
                              'AFCB0',
                              'AFCA0'),
                       Node),
         Node = ifelse(SiteID %in% c('TUCH', 'TFH'),
                       'TUCH_TFH',
                       Node),
         Node = ifelse(SiteID == 'MCCA',
                       'STR',
                       Node),
         Node = ifelse(SiteID == 'CARMEC',
                       'CRCA0',
                       Node),
         Node = ifelse(SiteID == 'BIG2C',
                       'TAYA0',
                       Node),
         Node = ifelse(SiteID == 'WIMPYC',
                       'WPCA0',
                       Node),
         Node = str_replace(Node, '^BTC', 'BTL'),
         Node = ifelse(SiteID %in% c('YANKFK', 'CEY'),
                       'YFKA0',
                       Node),
         Node = ifelse(SiteID == 'SAWT',
                       'STL',
                       Node),
         Node = ifelse(SiteID == 'LOOH',
                       'LOOKGC',
                       Node),
         Node = ifelse(SiteID == 'RPDTRP',
                       'RAPH',
                       Node),
         Node = ifelse(SiteID == 'CHARLC',
                       'CCAB0',
                       Node),
         Node = ifelse(Node == 'KEN',
                       'KENB0',
                       Node),
         Node = ifelse(Node == 'HYC',
                       'HYCB0',
                       Node),
         Node = ifelse(Node == 'LLR',
                       'LLRB0',
                       Node),
         Node = ifelse(Node == 'LRW',
                       'LRWB0',
                       Node),
         Node = ifelse(SiteID == '18M',
                       paste0('X', Node),
                       Node)) %>%
  distinct()

# add HBC if it isn't in there yet
if(!'HBC' %in% unique(my_config$SiteID)) {
  my_config = my_config %>%
    bind_rows(tibble(SiteID = 'HBC',
                     Node = 'HBC',
                     SiteType = 'INT',
                     RKM = '522.303.416.049.002',
                     RKMTotal = 1292))
}

# a sample of this configuration file
head(my_config)
```

## Node Network 

The next step is to define how the various sites that are to be used with DABOM are related to each other. `PITcleanr` contains a function to describe that network for the Lower Granite version of DABOM, `writeLGRNodeNetwork()`. This serves two purposes: it defines which sites are to be used in the DABOM model, and it describes how they are related to each other along the stream network.

```{r}
site_df = writeLGRNodeNetwork()

# remove some sites that have been combined with others (see the modifications to the configuration file)
site_df = site_df %>%
  filter(!SiteID %in% c('MCCA',
                        'WIMPYC',
                        'YANKFK', 'CEY',
                        'SAWT',
                        'LOOH',
                        'CARMEC',
                        'BIG2C',
                        'RPDTRP'))

# a sample of what this looks like
head(site_df)
```

## Parent-Child Table

Next, we build a parent-child dataframe describing which child nodes are directly upstream of each parent node in our system. The `createParentChildDf` function contains several arguements:

* `sites_df`: This is the dataframe we built using the `writeLGRNodeNetwork()` function.
* `config`: This is the configuration file we built using the `buildConfig()` function.
* `startSite`: This defines which site all paths will start from. This is typically the dam where the adult trap is located.
* `startDate`: The first date (in YYYYMMDD format) when fish may start being caught in the adult trap. Configurations that ended prior to this date are excluded.

```{r}
parent_child = createParentChildDf(site_df,
                                   my_config,
                                   startSite = 'GRA',
                                   startDate = ifelse(spp == 'Chinook',
                                                      paste0(yr, '0301'),
                                                      paste0(yr-1, '0701')))

# add one site in Kenney Creek, if it's been dropped
if(sum(grepl('KENA0', parent_child$ChildNode)) == 0) {
  lineNum = which(parent_child$ChildNode == 'KENB0')
  
  parent_child = parent_child %>%
    slice(1:lineNum) %>%
    bind_rows(parent_child %>%
                slice(lineNum) %>%
                mutate(ParentNode = 'KENB0',
                       ChildNode = 'KENA0')) %>%
    bind_rows(parent_child %>%
                slice((lineNum + 1):n()))
}

```

## Valid Tags

The user must define the path to the adult trap database, which contains the data `PITcleanr` uses to determine which tags are valid to be fed into DABOM. `PITcleanr` contains an fictional example of the trap database, modeled after the one maintained by IDFG. If the user has access to the trap database, set `trap_path` equal to the file path, including the file name and extension. Currently it must be in CSV format. If the user does not have a copy of the trap database, one must be obtained before running `PITcleanr`.

From the trap database, `PITcleanr` searches for tags that are part of the valid tag list for a particular species and year, and returns a dataframe with their tag code and the date of trapping. Currently, within the Lower Granite trap database, `PITcleanr` filters for tags of the appropriate species and spawn year, and then filters for returning adults (`LGDLifeStage == 'RF'`), adipose-intact fish (`LGDMarkAD == 'AI'`), with non-missing PIT tag numbers (`!is.na(LGDNumPIT)`) and that are marked as valid (`LGDValid == 1`). Besides filtering out these valid tags, the function `filterLGRtrapDB` provides an arguement to save those tag IDs as a text file, to make it easier to upload to PTAGIS. 

```{r}
valid_df = filterLGRtrapDB(trap_path = '../inst/extdata/Chnk2014_TrapDatabase.csv',
                           species = spp,
                           spawnYear = yr,
                           saveValidTagList = T,
                           validTagFileNm = 'ValidTags.txt')
```

## Complete Tag History

The next step is to compile the complete tag history of all valid tags in our sample. This can be done in two ways.

### Through PTAGIS

After logging into [PTAGIS](http://PTAGIS.org), select the "Data" tab, "Advanced Reporting" and then the "Launch" button. Now, select the "New Query Builder2 Report" icon and the "Complete Tag History" option. The __DABOM__ functions require the following attributes to be selected; *Tag*, *Event Date Time*, *Event Release Date Time*, *Event Site Code*, *Antenna*, and *Antenna Group Configuration*. Other attributes can be selected and they can appear in any order.  After selecting attributes, upload *ValidTags.txt* in the 'Tag Code - List or Text File' section and then run the report and save the output.  Next read the outputted observation file into R or use the example observation file, "chnk15_obs".

```{r}
data(chnk2015_obs)
observations = chnk2015_obs
```

### Through DART

DART has built an API query allowing users to query the complete tag history of any particular tag. Be warned however, on one test machine, this query could return the records of about 10 tags per minute, so for several thousand tags, it will take awhile. 

```{r, eval = F}
observations = valid_tag_df %>%
  select(TagID) %>%
  as.matrix() %>%
  as.character() %>%
  as.list() %>%
  map_df(.f = queryCapHist,
         configuration = my_config)

```

## Process Raw Observations

`PITcleanr` contains a single function, `processCapHist_LGD` that processes the raw observations and returns several key pieces of information. First, it constructs a table of all the pathways a fish might take that are "valid" in DABOM, meaning they reflect continual upstream movement along a single branch of the stream network (using an internal function, `getValidPaths`). Next, it uses the trap database to determine the trap date of each tag. The next step is to assign each observation to an appropriate node from the DABOM model, as defined in the configuration file, using an internal function, `assignNodes()`. It requires the complete tag histories, the dataframe of valid tags and their trap date, the site configuration dataframe, and the parent-child dataframe to filter out observations from sites not contained in the DABOM version. If `truncate` is set to `TRUE` (the default), the `assignNodes()` function will filter out observations that occurred prior to the trap date, and remove consecutive observations at the same node to simplify the file. 

Finally, the processing function determines whether each observation should be considered valid for `DABOM`. It does this by determining the final node a fish was observed at, and querying the valid path dataframe for all the downstream sites that may be encountered on the way to that node. It also proposes an extended path of the first set of nodes upstream of the final node, and acknowledges that observations along that extended path may also be valid (e.g. a fish swims a particular route upstream, spawns and then swims partway downstream, past a node or two. The most upstream node should be considered a valid observation, and the likely spawning location.).

It returns a dataframe with a row for each observed node, showing the tag code, the site code, the model node, the minimum observed date-time, and two columns called "AutoProcStatus" and "UserProcStatus". There are additional columns as well, called:

* `NodeOrder`: how many nodes would a tag cross, including the one identified in that row, to reach the current node? This is regardless of whether a tag was observed at those nodes.
* `Direction`: based on previously observed node, is the tag moving upstream or downstream? If this is `NA`, the current node is not along a valid path containing the previous node.
* `ValidPath`: Taken together, do all the observed nodes for a particular tag fall along a single valid path? 
* `ModelObs`: Marked `TRUE` for all observed nodes, in sequence by observation, that initially constitute a valid path. Does not assume that the final observation is along the true valid path.
* `SiteDescription`: The site description of the site, as provided by PTAGIS.

"AutoProcStatus" is `PITcleanr`'s best attempt to determine which observations should be kept (and which should be deleted) before bringing the data into DABOM. "UserProcStatus"" is there for the user to change things. Any tag that has at least one flagged observation ("AutoProcStatus" == `FALSE`) will have all of the observations labeled as blanks in the "UserProcStatus" column. This allows the user to filter initially on the rows with `UserProcStatus == ''`, and see all the observations for each flagged tag. The user can then determine which observations should be included as "valid" by marking the UserProcStatus as `TRUE`, or which ones should be deleted by marking the UserProcStatus as `FALSE`, guided by the suggestions in the AutoProcStatus column, information contained in the other columns, and by the user's knowledge of fish behavior and the system in question. The *UserComment* field is meant to record reasons why certain observations are deleted.

We imagine the workflow to involve initially filtering on all the blank UserProcStatus rows. Then, for each tag, determine whether each observation should be kept (mark *UserProcStatus* == `TRUE`) or discarded (mark *UserProcStatus* == 'FALSE`) based on suggestions from the *AutoProcStatus*, *ValidPaths* and *ModelObs* fields. For any observation destined to be discarded, a reason should be provided in the *UserComment* for re-producibility. When no blank *UserProcStatus* rows remain, save the file. It is now ready for importation into DABOM.


```{r}
proc_list = processCapHist_LGD(species = spp,
                               spawnYear = yr,
                               configuration = my_config,
                               parent_child,
                               trap_path = NULL,
                               filter_by_PBT = T,
                               observations = observations,
                               truncate = T,
                               save_file = T,
                               file_name = 'ProcessedCH.xlsx')
```

# Summarise final spawning location and biological information

`PITcleanr` provides a function to determine a fish's final spawning location, and combine it with lots of biological data from the trap database at Lower Granite, `summariseTagData`. It relies on a processed capture history file returned by the function `proc_list`. Internally, this relies on another function, `estimateSpawnLoc`, which determines the final spawning location of each tag, by finding the furthest upstream node a tag was observed at within it's valid observations.

```{r}
tag_summ = summariseTagData(capHist_proc = proc_list$ProcCapHist,
                            trap_data = proc_list$ValidTrapData)

head(tag_summ)
```

